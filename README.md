# LLM-Lab
“From Scratch to Alignment – Building and Fine-Tuning LLMs Step by Step.”

This repository provides a complete journey into Large Language Models (LLMs) — from building them from scratch to advanced fine-tuning techniques. It covers:

Pretraining – creating an LLM from initial training on large-scale text corpora.
Instruction Fine-Tuning – adapting models to follow human instructions effectively.
Preference Fine-Tuning (RLHF / DPO) – aligning models with human preferences for safer and more useful responses.

The repo is structured to serve as a learning resource, playground, and reference implementation for researchers, practitioners, and enthusiasts interested in understanding how modern LLMs are trained, tuned, and aligned.
